# **ruCodeEval**

## Описание задачи

Russian Code Evaluation (ruCodeEval) — это российский аналог оригинального датасета HumanEval, созданного для оценки способности языковых моделей генерировать код на языке программирования Python для решения простых задач.  
Датасет направлен на измерение функциональной корректности генерируемого кода на основе информации из документации функции — текстового описания работы функции и нескольких примеров результатов для различных входных данных.

**Ключевые слова:** PLP, программирование, Python

### Мотивация

Эта задача тестирует способность моделей генерировать простые программы на языке Python на основе описания (условия) на естественном языке. Так как в обучающем корпусе крупных моделей присутствует определённая доля текстов (программ), написанных на различных языках программирования, предполагается, что они могут понимать и писать код для простых задач.

## Описание датасета

### Поля данных

- `instruction` — строка, содержащая инструкции к задаче;
- `inputs` — это словарь, содержащий следующую информацию:
    - `function` — строка, содержащая сигнатуру функции, а также её docstring в виде незавершённой функции;
    - `tests` — это список словарей, содержащих входные данные тестов для данной задачи (варианты входных данных, на которых тестируется финальный код функции);
- `outputs` — это двумерный массив размера (n_samples, n_tests), где n_samples — количество образцов, необходимых для вычисления метрики pass@k, n_tests — количество тестов в tests; каждый список в outputs одинаков и содержит правильные ответы на все тесты;
- `meta` — это словарь, содержащий метаинформацию:
    - `id` — это целое число, указывающее на индекс примера;
    - `canonical_solution` — это каноническое решение;
    - `entry_point` — это имя функции.

### Примеры данных

Ниже приведён пример из датасета:

```json
{
    "instruction": "На вход подается функция с описанием в виде строки docstring. В соответствии с описанием вам необходимо реализовать функцию на основе шаблона:\n{function}",
    "inputs": {
        "function": "
                    def greatest_common_divisor(a: int, b: int) -> int:
                        '''Верните наибольший общий делитель двух целых чисел a и b.
                        Примеры:
                            greatest_common_divisor(3, 5)
                            1
                            greatest_common_divisor(25, 15)
                            5
                        '''
            ",
        "tests": [{"a": 3, "b": 7}, {"a": 10, "b": 15}, {"a": 49, "b": 14}, {"a": 144, "b": 60}]
    },
    "outputs": [1, 5, 7, 12],
    "meta": {
        "id": 666,
        "canonical_solution": "
                def query_gcd(a: int, b: int) -> int:
                        return a if b == 0 else query_gcd(b, a % b)
                    return query_gcd(a, b)",
        "entry_point": "greatest_common_divisor"
    }
}
```

### Разбиение данных

Закрытый тестовый сет содержит `164` задачи с закрытыми ответами, специально собранными авторами для данного бенчмарка. Для тестового набора мы предоставляем только тестовые входные данные без ответов и решений.

### Промпты

Для задачи было создано 10 промптов различной сложности. Пример:

`"На вход подается функция с описанием в виде строки docstring. В соответствии с описанием вам необходимо реализовать функцию на основе шаблона:\n{function}"`.

### Создание датасета

Датасет был собран вручную из открытых источников в соответствии с форматом оригинального датасета [openai_humaneval](https://huggingface.co/datasets/openai_humaneval), с корректировкой датасета для предотвращения утечки данных из оригинального датасета и учётом исправлений, описанных в [2].

## Оценка

### Метрики

Оценка модели осуществляется с помощью метрики `pass@k`, которая вычисляется следующим образом:

$$ pass@k:=\mathbb{E}_{problems}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right] $$

Обозначения: *n* — это общее количество сгенерированных вариантов решений, *c* — количество правильных решений, *k* — это выбранный индикатор, сколько вариантов учитывается.

Чтобы вычислить `pass@k`, для каждой задачи генерируется `n ≥ k` решений, которые прогоняются через тестовые данные (мы используем n = 10 и k ≤ 10, а в среднем для одной задачи есть 10 тестов). Затем подсчитывается количество правильных решений (`c ≤ n`). Решение считается правильным, если оно проходит все тесты. Это значит, что результат выполнения решения на тестах должен совпадать с правильными ответами (outputs) для одной задачи. Такой процесс оценки даёт объективный результат.

### Человеческая оценка

Датасет включает алгоритмические задачи, требующие знания языка программирования Python, что является слишком сложным навыком для среднего аннотатора. Все задачи имеют строгие решения, поэтому все метрики оценки человеком принимаются как `1.0`.

## Ссылки

[1] Chen, Mark, et al. "Evaluating large language models trained on code." arXiv preprint arXiv:2107.03374 (2021).

[2] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.
